{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b5a3be-bd29-4c23-921f-ecc4935cf5ac",
   "metadata": {},
   "source": [
    "### 1. Importing Packages and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d659a-d9ff-4c4c-93de-06edbfa80718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a69ed-3cda-4c00-ba2d-ea0d160e99b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './iranian+churn+dataset.zip'\n",
    "\n",
    "with ZipFile(file_path) as z:\n",
    "    print(z.namelist())\n",
    "\n",
    "filename = z.namelist()[0]\n",
    "with ZipFile(file_path).open(filename) as f:\n",
    "    df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741da55a-5c24-4d49-b548-7469ab378974",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Age'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dfaa9c-39f4-4508-b8ec-a80c8927431c",
   "metadata": {},
   "source": [
    "### 2. Splitting Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4837bf24-9807-4888-8f2e-940c6d49895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = ['Churn']) # dataframe\n",
    "y = df['Churn'] # series\n",
    "\n",
    "# Use stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=312, stratify=y)\n",
    "\n",
    "print(f'# of train samples: {len(X_train)}')\n",
    "print(f'# of test samples: {len(X_test)}')\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ec5c40-aded-4e26-89de-3d516d2bc830",
   "metadata": {},
   "source": [
    "### 3. Tree Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48798f15-94ec-4d25-8d11-21898ddc5d42",
   "metadata": {},
   "source": [
    "#### 3.1 Setting Custom Cross-Validation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921219f6-55f4-4217-8036-62bfb0f70e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_list = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "\n",
    "def matrix_to_metrics(mat):\n",
    "    TP = mat[1, 1]\n",
    "    TN = mat[0, 0]\n",
    "    FP = mat[0, 1]\n",
    "    FN = mat[1, 0]\n",
    "    \n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    prec = TP / (TP + FP) \n",
    "    rec = TP / (TP + FN)\n",
    "    f1_score = 2 * (prec * rec) / (prec + rec)\n",
    "    \n",
    "    return [acc, prec, rec, f1_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c6d4a-0e92-4848-8cc2-f240eb894214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48791fb1-ec59-45ef-80b5-1de140cba81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0d19f-95b5-48b2-9a49-a152215fac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(data, model_class, model_params, drops):\n",
    "    result_dict = {} \n",
    "    feature_importances_list = []\n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    for m in metrics:\n",
    "        result_dict[m] = []\n",
    "\n",
    "    X_train, y_train = data\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    X_train = X_train.drop(columns=drops, axis=1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for fold in kf.split(X_train): \n",
    "        trn_idx, val_idx = fold\n",
    "        X_trn, X_val = X_train.iloc[trn_idx], X_train.iloc[val_idx] # analysis set \n",
    "        y_trn, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx] # assessment set\n",
    "        \n",
    "        # model fitting\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_trn, y_trn)\n",
    "\n",
    "        # feature importance\n",
    "        feature_importances_list.append(model.feature_importances_)\n",
    "        \n",
    "        # model evaluation\n",
    "        y_pred = model.predict(X_val)\n",
    "        conf_mat = confusion_matrix(y_val, y_pred)\n",
    "        fold_result = matrix_to_metrics(conf_mat)\n",
    "        for i, val in enumerate(fold_result):\n",
    "            result_dict[metrics[i]].append(round(fold_result[i], 4))\n",
    "            \n",
    "    end_time = time.time() \n",
    "    elapsed_time = end_time - start_time\n",
    "    readable_time = f\"{elapsed_time:.3f} seconds\"\n",
    "    print(readable_time)\n",
    "\n",
    "    for key in result_dict:\n",
    "        result_dict[key].append({'average': round(sum(result_dict[key]) / len(result_dict[key]), 3)})\n",
    "\n",
    "    average_importances = np.mean(feature_importances_list, axis=0)\n",
    "    importances_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': average_importances\n",
    "    })\n",
    "    importances_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "        \n",
    "    return result_dict, importances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60daf4c2-20c4-45f8-a278-b1c8835b65b9",
   "metadata": {},
   "source": [
    "#### 3.2 Preliminary Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e429df-a089-4313-be85-49c6d09b42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(result_dict):\n",
    "    cols = []\n",
    "    avgs = []\n",
    "    for key in result_dict:\n",
    "        cols.append(key)\n",
    "        avgs.append(result_dict[key][-1]['average'])\n",
    "    return pd.DataFrame({'Metric': cols, 'Average': avgs}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76079b60-8415-47ee-b7f9-99b995e96cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(df_dict): \n",
    "    n = len(df_dict)\n",
    "    for idx, item in enumerate(df_dict.items()):\n",
    "        model_name, result = item\n",
    "        if idx == 0:\n",
    "            merged_df = result.rename(columns={'Average': model_name})\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, result.rename(columns={'Average': model_name}), on='Metric')\n",
    "\n",
    "    merged_df = merged_df.set_index('Metric')\n",
    "    merged_df['best_model'] = merged_df.idxmax(axis=1)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ba69f-749c-4544-8a14-74178d90eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class_dict = {}\n",
    "model_class_dict['DecisionTree'] = DecisionTreeClassifier\n",
    "model_class_dict['RandomForest'] = RandomForestClassifier\n",
    "model_class_dict['ExtraTrees'] = ExtraTreesClassifier\n",
    "model_class_dict['XGBoost'] =  XGBClassifier\n",
    "model_class_dict['LightGBM'] = LGBMClassifier\n",
    "model_class_dict['CatBoost'] = CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b54684-55d5-457d-9135-72628f86e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tree(model_dict):\n",
    "    result_dict = {}\n",
    "    feat_info = {}\n",
    "    for name in model_dict:\n",
    "        print(name, '='* (20-len(name)))\n",
    "        data = model_dict[name]['data']\n",
    "        model_class = model_dict[name]['class']\n",
    "        model_params= model_dict[name]['param']\n",
    "        drops = model_dict[name]['drop']\n",
    "        single_result, single_feat_info = cross_validate(data, model_class, model_params, drops)\n",
    "        result_dict[name] = get_summary(single_result)   \n",
    "        feat_info[name] = single_feat_info\n",
    "    return result_dict, feat_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957c7ba-fd0d-4f2b-84fe-9a8b2f007a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict = {}\n",
    "for mc in model_class_dict:\n",
    "    tree_dict[mc] = {}\n",
    "    cur_dict = tree_dict[mc]    \n",
    "    cur_dict['data'] = [X_train, y_train]\n",
    "    cur_dict['class'] = model_class_dict[mc]\n",
    "    if mc == 'CatBoost':\n",
    "        cur_dict['param'] = {'verbose': False}\n",
    "    else:\n",
    "        cur_dict['param'] = {}\n",
    "    cur_dict['drop'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1fccc0-e297-46d3-a60f-8cf5a94092d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_results, tree_feats = run_tree(tree_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e6b28-a679-488a-8650-3818f1423326",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(tree_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f6ab3-42a4-45f9-b63e-a6f04cf2d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_feats['LightGBM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb2f1d5-562c-4f8f-bd2a-498dd87a96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing variables with zero importance\n",
    "lgbm_wrapper = {}\n",
    "lgbm_wrapper['drop'] = {}\n",
    "cur_dict = lgbm_wrapper['drop']   \n",
    "cur_dict['data'] = [X_train, y_train]\n",
    "cur_dict['class'] = LGBMClassifier\n",
    "cur_dict['param'] = {}\n",
    "cur_dict['drop'] = ['Tariff Plan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c54a3-4f01-4187-bb4f-9715b4d3e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_results, lgbm_feats = run_tree(lgbm_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70895394-c341-4b34-88ae-6a530a555cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_results['drop'] # classification performance remained unchanged after removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c25b4b-b644-474e-afe9-7a4bc53e648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_feats['drop'] # variable importance remained unchanged after removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9521c62-e41c-4dc0-81f0-68edcb543326",
   "metadata": {},
   "source": [
    "#### 3.3 Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50e2b2d-7f76-4d1f-9a51-a1d3ee222505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE\n",
    "def smote_cross_validate(data, model_class, model_params, drops):\n",
    "    result_dict = {} \n",
    "    feature_importances_list = []\n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    for m in metrics:\n",
    "        result_dict[m] = []\n",
    "\n",
    "    X_train, y_train = data\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    X_train = X_train.drop(columns=drops, axis=1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for fold in kf.split(X_train): \n",
    "        trn_idx, val_idx = fold\n",
    "        X_trn, X_val = X_train.iloc[trn_idx], X_train.iloc[val_idx] # analysis set \n",
    "        y_trn, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx] # assessment set\n",
    "\n",
    "        # smote\n",
    "        cate = ['Complains', 'Charge  Amount', 'Age Group', 'Status']\n",
    "        smote_nc = SMOTENC(categorical_features=cate, random_state=42)\n",
    "        X_trn, y_trn = smote_nc.fit_resample(X_trn, y_trn)\n",
    "        \n",
    "        # model fitting\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_trn, y_trn)\n",
    "\n",
    "        # feature importance\n",
    "        feature_importances_list.append(model.feature_importances_)\n",
    "        \n",
    "        # model evaluation\n",
    "        y_pred = model.predict(X_val)\n",
    "        conf_mat = confusion_matrix(y_val, y_pred)\n",
    "        fold_result = matrix_to_metrics(conf_mat)\n",
    "        for i, val in enumerate(fold_result):\n",
    "            result_dict[metrics[i]].append(round(fold_result[i], 4))\n",
    "            \n",
    "    end_time = time.time() \n",
    "    elapsed_time = end_time - start_time\n",
    "    readable_time = f\"{elapsed_time:.3f} seconds\"\n",
    "    print(readable_time)\n",
    "\n",
    "    for key in result_dict:\n",
    "        result_dict[key].append({'average': round(sum(result_dict[key]) / len(result_dict[key]), 3)})\n",
    "\n",
    "    average_importances = np.mean(feature_importances_list, axis=0)\n",
    "    importances_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': average_importances\n",
    "    })\n",
    "    importances_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "        \n",
    "    return result_dict, importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7dbb9-008a-4c78-80bf-654d2d468940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "def run_smote(model_dict):\n",
    "    result_dict = {}\n",
    "    feat_info = {}\n",
    "    for name in model_dict:\n",
    "        print(name, '='* (20-len(name)))\n",
    "        data = model_dict[name]['data']\n",
    "        model_class = model_dict[name]['class']\n",
    "        model_params= model_dict[name]['param']\n",
    "        drops = model_dict[name]['drop']\n",
    "        single_result, single_feat_info = smote_cross_validate(data, model_class, model_params, drops)\n",
    "        result_dict[name] = get_summary(single_result)   \n",
    "        feat_info[name] = single_feat_info\n",
    "    return result_dict, feat_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc0fc8-779a-4fa8-9d83-2fe6864b0430",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_smote_results, lgbm_smote_feats = run_smote(lgbm_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22eb60-9b82-4bf2-8342-168da7aab243",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_smote_results['drop'] # recall increased, but all other metrics declined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae515f13-4199-4805-abf1-484b1efb1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_smote_feats['drop'] # variable importance varies with smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b4a5c-cb09-4e5c-ac04-f4b9a8e35234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Weight Adjustment\n",
    "lgbm_weight_wrapper = {}\n",
    "lgbm_weight_wrapper['drop'] = {}\n",
    "cur_dict = lgbm_weight_wrapper['drop']   \n",
    "cur_dict['data'] = [X_train, y_train]\n",
    "cur_dict['class'] = LGBMClassifier\n",
    "cur_dict['param'] = {'is_unbalance': True}\n",
    "cur_dict['drop'] = ['Tariff Plan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9a683-7b51-4fac-932a-9695bbb7637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_weight_results, lgbm_weight_feats = run_tree(lgbm_weight_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9714f-8282-48ea-a788-39136284f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_weight_results['drop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06c5e5-6028-41ce-8633-b2cd1246e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_weight_feats['drop']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e8d55-ea7a-4841-a314-cd7d88cc8865",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3.4 Summary of CV Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065c993-7263-4666-b33d-924f37d0a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(df_dict):\n",
    "    merged_df = pd.DataFrame()\n",
    "    for key, df in df_dict.items():\n",
    "        df_renamed = df.rename(columns={'Average': key})\n",
    "        if merged_df.empty:\n",
    "            merged_df = df_renamed\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df_renamed, on='Metric')\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f6237-4f51-4f74-90e2-a91221552ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = compare_models(tree_results).drop(columns=['best_model'])\n",
    "df_dict = {}\n",
    "df_dict['drop'] = lgbm_results['drop']\n",
    "df_dict['drop_smote'] = lgbm_smote_results['drop']\n",
    "df_dict['drop_weight'] = lgbm_weight_results['drop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658d442-9b3b-431a-9449-72493655a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize(df_dict)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02132823-3c8b-43d4-b382-4628250d875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "dfs = [trees, summary]\n",
    "merged_df_final = reduce(lambda left, right: pd.merge(left, right, on='Metric'), dfs)\n",
    "merged_df_final.set_index('Metric', inplace=True)\n",
    "merged_df_final['best_model'] = merged_df_final.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e87a7-4e31-4cc4-a755-d6eb843cd871",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd80da29-104b-4c12-a5ae-af01d8387fc7",
   "metadata": {},
   "source": [
    "#### 3.5 Threshold Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aef3cd-6df3-482a-9e25-1dc599c1f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde1a4d-1d5b-4167-bd1d-33d6b3902d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curve_cross_validate(data, model_class, model_params, drops):\n",
    "    result_dict = {} \n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    for m in metrics:\n",
    "        result_dict[m] = []\n",
    "\n",
    "    X_train, y_train = data\n",
    "    X_train = X_train.drop(columns=drops, axis=1)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for fold_idx, fold in enumerate(kf.split(X_train)): \n",
    "        trn_idx, val_idx = fold\n",
    "        X_trn, X_val = X_train.iloc[trn_idx], X_train.iloc[val_idx] # analysis set \n",
    "        y_trn, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx] # assessment set\n",
    "        \n",
    "        # model fitting\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_trn, y_trn)\n",
    "        \n",
    "        # model evaluation\n",
    "        y_pred = model.predict(X_val)\n",
    "        conf_mat = confusion_matrix(y_val, y_pred)\n",
    "        fold_result = matrix_to_metrics(conf_mat)\n",
    "        for i, val in enumerate(fold_result):\n",
    "            result_dict[metrics[i]].append(round(fold_result[i], 4))\n",
    "\n",
    "        y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "        # ROC 곡선과 AUC 계산\n",
    "        fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        \n",
    "        # ROC 커브 그리기\n",
    "        plt.plot(fpr, tpr, lw=2, alpha=0.3, label=f'Fold {fold_idx+1} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "        # 보간법으로 TPR 저장 (평균 ROC 계산에 사용)\n",
    "        tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "\n",
    "        # 최적 임계값 선택\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        print(f\"\\nOptimal threshold based on ROC curve: {optimal_threshold:.2f}\")\n",
    "        \n",
    "        # 최적 임계값으로 예측\n",
    "        y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "        print(\"\\nClassification report with optimal threshold:\\n\")\n",
    "        print(classification_report(y_val, y_pred_optimal))\n",
    "            \n",
    "    end_time = time.time() \n",
    "    elapsed_time = end_time - start_time\n",
    "    readable_time = f\"{elapsed_time:.3f} seconds\"\n",
    "    print(readable_time)\n",
    "\n",
    "    for key in result_dict:\n",
    "        result_dict[key].append({'average': round(sum(result_dict[key]) / len(result_dict[key]), 3)})\n",
    "\n",
    "    # 평균 ROC 곡선 및 AUC 계산\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, color='b', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=2)\n",
    "    \n",
    "    # 대각선 기준선 그리기\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8)\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC-AUC Curves for Each Fold')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "        \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996557e6-a341-4e57-bf58-648ef5f4ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_threshold(model_dict):\n",
    "    result_dict = {}\n",
    "    feat_info = {}\n",
    "    for name in model_dict:\n",
    "        print(name, '='* (20-len(name)))\n",
    "        data = model_dict[name]['data']\n",
    "        model_class = model_dict[name]['class']\n",
    "        model_params= model_dict[name]['param']\n",
    "        drops = model_dict[name]['drop']\n",
    "        single_result = curve_cross_validate(data, model_class, model_params, drops)\n",
    "        result_dict[name] = get_summary(single_result)   \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c48d57-b07d-4ac9-87dc-a9b3ec5b3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_wrapper = {}\n",
    "lgbm_wrapper['drop'] = {}\n",
    "cur_dict = lgbm_wrapper['drop']   \n",
    "cur_dict['data'] = [X_train, y_train]\n",
    "cur_dict['class'] = LGBMClassifier\n",
    "cur_dict['param'] = {}\n",
    "cur_dict['drop'] = ['Tariff Plan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665843b-b656-4746-a56a-d4340bd9dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_result = run_threshold(lgbm_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f062a7-a97f-4f84-8a6f-34db899be73c",
   "metadata": {},
   "source": [
    "#### 3.6 Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7860e5-98b4-45a8-95a9-9652d8441c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, model_class, model_params, drops):\n",
    "    result_dict = {} \n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    for m in metrics:\n",
    "        result_dict[m] = []\n",
    "\n",
    "    [X_train, y_train, X_test, y_test] = data\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    model = model_class(**model_params)\n",
    "    model.fit(X_train, y_train)\n",
    "        \n",
    "    #y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    # 임계값을 적용하여 최종 예측\n",
    "    threshold = 0.2\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    final_result = matrix_to_metrics(conf_mat)\n",
    "    for i, val in enumerate(final_result):\n",
    "        result_dict[metrics[i]].append(round(final_result[i], 4))\n",
    "        \n",
    "    end_time = time.time() \n",
    "    elapsed_time = end_time - start_time\n",
    "    readable_time = f\"{elapsed_time:.3f} seconds\"\n",
    "    print(readable_time)\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13cb4a3-62c3-4ea4-a166-a5f088754e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model_dict):\n",
    "    result_dict = {}\n",
    "    for name in model_dict:\n",
    "        print(name, '='* (20-len(name)))\n",
    "        data = model_dict[name]['data']\n",
    "        model_class = model_dict[name]['class']\n",
    "        model_params= model_dict[name]['param']\n",
    "        drops = model_dict[name]['drop']\n",
    "        single_result = train(data, model_class, model_params, drops)\n",
    "        result_dict[name] = single_result \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf6832a-7ed2-491c-96a5-7e8d5ea5d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_wrapper = {}\n",
    "lgbm_wrapper['drop'] = {}\n",
    "cur_dict = lgbm_wrapper['drop']   \n",
    "cur_dict['data'] = [X_train, y_train, X_test, y_test]\n",
    "cur_dict['class'] = LGBMClassifier\n",
    "cur_dict['param'] = {}\n",
    "cur_dict['drop'] = ['Tariff Plan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25925290-0887-422f-8b44-2c030c771cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = run(lgbm_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0206e-e8b9-4e40-8c85-383e88b0b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(final_result['drop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df36836-74d1-4d2d-9c4e-15baf06cb36b",
   "metadata": {},
   "source": [
    "### 4. Neural Net "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc71f42-9eac-45ed-a809-db27b2f74cf1",
   "metadata": {},
   "source": [
    "#### 4.1 Vanilla Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3042b1df-fe51-4811-aec9-d1393fb9a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08039e4a-8fbe-44be-9200-a151575923a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassificationNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BinaryClassificationNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 8)\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407cc1b2-7640-40ca-9f32-a736b2979e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cross_validate(data, trainer, drops, scaler):\n",
    "    result_dict = {} \n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    for m in metrics:\n",
    "        result_dict[m] = []\n",
    "\n",
    "    X_train, y_train = data\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # removing correlated columns\n",
    "    X_train = X_train.drop(columns=drops, axis=1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, fold in enumerate(kf.split(X_train)): \n",
    "        trn_idx, val_idx = fold\n",
    "        X_trn, X_val = X_train.iloc[trn_idx], X_train.iloc[val_idx] # analysis set \n",
    "        y_trn, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx] # assessment set\n",
    "\n",
    "        # scaling\n",
    "        if scaler is not None:   \n",
    "            # previous parameters not retained\n",
    "            X_trn = scaler.fit_transform(X_trn)  \n",
    "            X_val = scaler.transform(X_val) \n",
    "            \n",
    "        # converting to tensor\n",
    "        X_trn = torch.tensor(X_trn, dtype=torch.float32) \n",
    "        X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_trn = torch.tensor(y_trn.to_numpy(), dtype= torch.float32).view(-1, 1)\n",
    "\n",
    "        # data loading\n",
    "        train_dataset = TensorDataset(X_trn, y_trn)\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        val_dataset = TensorDataset(X_val)\n",
    "        val_loader = DataLoader(dataset=val_dataset, batch_size=32)\n",
    "\n",
    "        # model fitting\n",
    "        fold_train = trainer(BinaryClassificationNet, input_dim=len(X_train.columns))\n",
    "        model = fold_train.model\n",
    "        model.train() \n",
    "\n",
    "        for epoch in range(fold_train.epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch in train_loader:\n",
    "                x_batch, y_batch = batch\n",
    "                fold_train.optimizer.zero_grad()\n",
    "                outputs = model(x_batch)\n",
    "                loss = fold_train.criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                fold_train.optimizer.step() \n",
    "                epoch_loss += loss.item()\n",
    "            if (epoch + 1) != 0 and (epoch + 1) % 10 == 0:\n",
    "                print(f'Fold [{idx+1}/5], Epoch [{epoch+1}/{fold_train.epochs}], Loss: {epoch_loss/len(train_loader):.4f}')\n",
    "        \n",
    "        # model evaluation\n",
    "        preds = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                outputs = model(batch[0])\n",
    "                pred = (outputs > 0.5).float().numpy().ravel() \n",
    "                preds.extend(pred)\n",
    "                \n",
    "        y_pred = np.array(preds)\n",
    "        conf_mat = confusion_matrix(y_val, y_pred)\n",
    "        fold_result = matrix_to_metrics(conf_mat)\n",
    "        for i, val in enumerate(fold_result):\n",
    "            result_dict[metrics[i]].append(round(fold_result[i], 4))\n",
    "            \n",
    "    end_time = time.time() \n",
    "    elapsed_time = end_time - start_time\n",
    "    readable_time = f\"{elapsed_time:.3f} seconds\"\n",
    "    print(readable_time)\n",
    "\n",
    "    for key in result_dict:\n",
    "        result_dict[key].append({'average': round(sum(result_dict[key]) / len(result_dict[key]), 3)})\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab4983-a09a-4ae3-bb17-1267b8079d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def customize_scaler(to_scale, to_skip):\n",
    "    scaler = ColumnTransformer(\n",
    "                transformers=[('process', StandardScaler(), to_scale), \n",
    "                    ('skip', 'passthrough', to_skip)])\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21423a82-715d-4090-bc1d-cbc0f0092f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "binaries = ['Complains', 'Tariff Plan', 'Status']\n",
    "uses = [c for c in X_train.columns if c not in binaries]\n",
    "base_scaler = customize_scaler(uses, binaries)\n",
    "base_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903ed7f-9819-4711-8f1d-f5f2135b176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassificationNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BinaryClassificationNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 8)\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a7bb2-4e16-424f-8043-394ad9aaf19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, input_dim):\n",
    "        self.model = model(input_dim)\n",
    "        self.epochs = 50\n",
    "        self.lr = 0.001\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061cb776-ce49-433c-bea6-d181f649a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nn(model_dict):\n",
    "    result_dict = {}\n",
    "    for name in model_dict:\n",
    "        print(name, '='* (20-len(name)))\n",
    "        data = model_dict[name]['data']\n",
    "        trainer = model_dict[name]['trainer']\n",
    "        drops = model_dict[name]['drops']\n",
    "        scaler = model_dict[name]['scaler']\n",
    "        single_result = nn_cross_validate(data, trainer, drops, scaler)\n",
    "        result_dict[name] = get_summary(single_result)   \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bef54d8-b85e-418d-9c68-42ce1659e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_wrapper = {'vanilla':{}}\n",
    "cur_dict = nn_wrapper['vanilla']\n",
    "cur_dict['data'] = [X_train, y_train]\n",
    "cur_dict['trainer'] = Trainer\n",
    "cur_dict['drops'] = []\n",
    "cur_dict['scaler'] = base_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb68b8-5178-4385-b89c-80612c4ba407",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_result = run_nn(nn_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a8a84-1ac4-43ec-b0b5-f95c13b52d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_result['vanilla']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88afb0b1-1d95-4463-be1b-7ca85278afa9",
   "metadata": {},
   "source": [
    "#### 4.2 Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba8d08-00ce-4c7f-aa26-700e6025d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d359a5a-de32-4a11-a4fd-1d2b6b516ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cv(lr, batch_size, X_train, y_train):\n",
    "    result_dict = {} \n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    for m in metrics:\n",
    "        result_dict[m] = []\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # setting scaler\n",
    "    binaries = ['Complains', 'Tariff Plan', 'Status']\n",
    "    uses = [c for c in X_train.columns if c not in binaries]\n",
    "    scaler = customize_scaler(uses, binaries)\n",
    "\n",
    "    # setting loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for idx, fold in enumerate(kf.split(X_train)): \n",
    "        trn_idx, val_idx = fold\n",
    "        X_trn, X_val = X_train.iloc[trn_idx], X_train.iloc[val_idx] # analysis set \n",
    "        y_trn, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx] # assessment set\n",
    "\n",
    "        # setting sampler\n",
    "        y_trn_tensor = torch.tensor(y_trn.values, dtype=torch.long)\n",
    "        class_sample_counts = torch.bincount(y_trn_tensor)  \n",
    "        weights = 1.0 / class_sample_counts.float()\n",
    "        sample_weights = weights[y_trn_tensor]  \n",
    "        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "        # scaling\n",
    "        # previous parameters not retained\n",
    "        X_trn = scaler.fit_transform(X_trn)  \n",
    "        X_val = scaler.transform(X_val) \n",
    "            \n",
    "        # converting to tensor\n",
    "        X_trn = torch.tensor(X_trn, dtype=torch.float32) \n",
    "        X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_trn = torch.tensor(y_trn.to_numpy(), dtype= torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "        # data loading\n",
    "        train_dataset = TensorDataset(X_trn, y_trn)\n",
    "        train_loader = DataLoader(dataset=train_dataset, sampler=sampler, batch_size=batch_size)\n",
    "        \n",
    "        val_dataset = TensorDataset(X_val)\n",
    "        val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size)\n",
    "\n",
    "        # model fitting\n",
    "        model = BinaryClassificationNet(input_dim=len(X_train.columns))\n",
    "        model.train() \n",
    "        \n",
    "        optimizer =  torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(50):\n",
    "            epoch_loss = 0\n",
    "            for batch in train_loader:\n",
    "                x_batch, y_batch = batch\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step() \n",
    "                epoch_loss += loss.item()\n",
    "            #if (epoch + 1) != 0 and (epoch + 1) % 10 == 0:\n",
    "                #print(f'Fold [{idx+1}/5], Epoch [{epoch+1}/{50}], Loss: {epoch_loss/len(train_loader):.4f}')\n",
    "        \n",
    "        # model evaluation\n",
    "        preds = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                outputs = model(batch[0])\n",
    "                pred = (outputs > 0.5).float().numpy().ravel() \n",
    "                preds.extend(pred)\n",
    "                \n",
    "        y_pred = np.array(preds)\n",
    "        conf_mat = confusion_matrix(y_val, y_pred)\n",
    "        fold_result = matrix_to_metrics(conf_mat)\n",
    "        for i, val in enumerate(fold_result):\n",
    "            result_dict[metrics[i]].append(round(fold_result[i], 4))\n",
    "\n",
    "    for key in result_dict:\n",
    "        result_dict[key].append({'average': round(sum(result_dict[key]) / len(result_dict[key]), 3)})\n",
    "\n",
    "    return result_dict['f1_score'][-1]['average']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d5c21-0275-49fa-b3cc-2aadb930f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_train, y_train):\n",
    "    # 샘플링할 하이퍼파라미터 정의\n",
    "    lr = trial.suggest_categorical('lr', [1e-4, 1e-3, 1e-2, 1e-1])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "\n",
    "    mean_val_score = nn_cv(lr, batch_size, X_train, y_train)\n",
    "    \n",
    "    return mean_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a10072-bf2f-41ec-a13f-4a49dd48ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(study_name='ann_tuning', direction='maximize', sampler=TPESampler(seed=42))\n",
    "study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=10)\n",
    "\n",
    "print()\n",
    "print(\"Best Score:\", study.best_value)\n",
    "print(\"Best trial:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc832db-4587-4488-9263-59229355a874",
   "metadata": {},
   "source": [
    "#### 4.3 Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff7344-aa23-4ddc-860e-1f1988e70643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr, batch_size, X_train, y_train, X_test, y_test):\n",
    "    result_dict = {} \n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    for m in metrics:\n",
    "        result_dict[m] = []\n",
    "\n",
    "    # setting scaler\n",
    "    binaries = ['Complains', 'Tariff Plan', 'Status']\n",
    "    uses = [c for c in X_train.columns if c not in binaries]\n",
    "    scaler = customize_scaler(uses, binaries)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # setting sampler\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    class_sample_counts = torch.bincount(y_train_tensor)  \n",
    "    weights = 1.0 / class_sample_counts.float()\n",
    "    sample_weights = weights[y_train_tensor]  \n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    # scaling\n",
    "    # previous parameters not retained\n",
    "    X_trn = scaler.fit_transform(X_train)  \n",
    "    X_test = scaler.transform(X_test) \n",
    "        \n",
    "    # converting to tensor\n",
    "    X_trn = torch.tensor(X_trn, dtype=torch.float32) \n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_trn = torch.tensor(y_train.to_numpy(), dtype= torch.float32).view(-1, 1)\n",
    "\n",
    "    # data loading\n",
    "    train_dataset = TensorDataset(X_trn, y_trn)\n",
    "    train_loader = DataLoader(dataset=train_dataset, sampler=sampler, batch_size=batch_size)\n",
    "    \n",
    "    test_dataset = TensorDataset(X_test)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # model fitting\n",
    "    model = BinaryClassificationNet(input_dim=len(X_train.columns))\n",
    "    model.train() \n",
    "    \n",
    "    optimizer =  torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            x_batch, y_batch = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            epoch_loss += loss.item()\n",
    "        if (epoch + 1) != 0 and (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{50}], Loss: {epoch_loss/len(train_loader):.4f}')\n",
    "    \n",
    "    # model evaluation\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            outputs = model(batch[0])\n",
    "            pred = (outputs > 0.5).float().numpy().ravel() \n",
    "            preds.extend(pred)\n",
    "            \n",
    "    y_pred = np.array(preds)\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    fold_result = matrix_to_metrics(conf_mat)\n",
    "    for i, val in enumerate(fold_result):\n",
    "        result_dict[metrics[i]].append(round(fold_result[i], 4))\n",
    "\n",
    "    for key in result_dict:\n",
    "        result_dict[key].append({'average': round(sum(result_dict[key]) / len(result_dict[key]), 3)})\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acc8b91-89cb-4aec-bfe5-1be3e234c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = train(0.01, 32, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c6cdef-8a38-400c-9a22-7b13df2f587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_summary(final_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
